 주제: Vanishing Gradient 문제(사라지는 기울기 문제)란?
✅ 문제가 뭐냐면:
신경망(머신러닝에서 뇌처럼 생각하는 모델)을 공부시키려고 하면,
처음에는 숫자를 조금씩 바꿔서 정답을 잘 맞추도록 배우게 해.

이때 "얼마나 바꿀지" 알려주는 게 기울기(gradient) 라는 거야.

✅ 근데 문제가 생겼어!
어떤 함수(=시그모이드 함수)를 쓰면, 계산할 때 결과가 0~1 사이의 숫자로 나와.

그리고 기울기도 1보다 작은 숫자가 돼.

✅ 왜 문제냐면?
뒤로 갈수록(=네트워크의 처음 쪽으로 갈수록)

1보다 작은 걸 계속 곱하고 또 곱하고 또 곱하면...

숫자가 너무 작아져서 거의 0이 돼버려!

이렇게 되면 앞쪽(처음층)에서는 배우질 못해!

✅ 쉽게 예를 들어서!
👦 “0.5를 계속 곱해볼까?”

0.5 × 0.5 = 0.25

0.25 × 0.5 = 0.125

0.125 × 0.5 = 0.0625 ...

점점 작아져서 거의 0이 돼.
➡️ 앞쪽 층이 바뀌지 않아서 배우는 속도가 너무 느려!

✅ 결과는?
앞쪽 층이 배우질 못하니까,

전체 공부가 너무 느리고,

정답 맞추는 능력도 떨어져.

✅ 그래서 어떻게 해?
시그모이드 같은 함수는 잘 안 써!

대신 ReLU(렐루) 같은 다른 함수를 써서 이 문제를 피하려고 해.

✅ 아주 짧게 요약!
너무 작은 수를 계속 곱하면 0에 가까워져서 앞쪽이 공부를 못해요. 이걸 사라지는 기울기 문제(Vanishing Gradient Problem) 라고 해요.