💡 주제: 활성화 함수(Activation Function)가 뭐야?
✅ 활성화 함수가 하는 일
신경망(머신러닝에서 뇌처럼 생각하는 모델)이 공부할 때,
👉 “어떻게 반응할지”를 정해 주는 역할을 해.

뇌에서 신호가 켜질지 말지를 결정하는 스위치 같아!

✅ 왜 중요해?
이 함수가 좋아야 컴퓨터가 잘 배우고 정답을 잘 맞출 수 있어!

나쁜 함수를 쓰면 잘 못 배워서 틀릴 수 있어.

✅ 옛날에 많이 쓴 함수: 시그모이드(Sigmoid) 함수
모양이 S자처럼 생겼어.

결과가 0~1 사이 숫자야.

입력이 크면 → 1에 가까움

입력이 작으면 → 0에 가까움

⚠️ 근데 문제가 있어!
1️⃣ 큰 숫자나 작은 숫자에서는 결과가 거의 변하지 않아.

계산이 평평해져서 배우기 힘들어.
2️⃣ “사라지는 기울기 문제(Vanishing Gradient)”가 생겨.

앞쪽 층이 너무 느리게 배우거나 거의 못 배워.

✅ 또 다른 문제
0~1 사이 값만 나오니까 항상 양수야!

다음 층으로 넘기는 값이 전부 +라서, 다양성이 떨어져.

✅ 이걸 고친 함수: 하이퍼볼릭 탄젠트(tanh)
시그모이드를 -1~+1로 늘린 버전.

0을 기준으로 대칭이야!

좋은 점: 양수도, 음수도 나옴.

⚠️ 하지만 여전히 깊은 신경망에서는 “사라지는 기울기” 문제가 있어.

✅ 요즘 제일 많이 쓰는 함수: ReLU(렐루)
아주 간단해!

0 이하면 → 0

0보다 크면 → 그대로!

장점:

계산 빠름

“사라지는 기울기” 문제를 잘 해결

몇 개 뉴런만 켜져서 효율적(스파스함)

✅ 마지막 함수: 소프트맥스(Softmax)
분류 문제 마지막 단계에서 사용!

여러 결과 중 어떤 게 제일 맞는지 확률처럼 계산해 줘.

예를 들어:

출력이 1.6, 0.55, 0.98이면

소프트맥스 통과 후 → 0.51, 0.18, 0.31 (합이 1!)

✅ 아주 간단한 정리!
활성화 함수는 신경망이 “어떻게 반응할지” 정하는 스위치야.
옛날에는 시그모이드를 썼는데, 배우기 어려운 문제(사라지는 기울기)가 있었어.
요즘은 렐루를 제일 많이 쓰고, 분류할 때는 마지막에 소프트맥스를 써!