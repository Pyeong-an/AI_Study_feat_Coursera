import wget
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
import torch

import numpy as np
import random
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
from transformers import AutoTokenizer, AutoModelForCausalLM


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.manifold import TSNE
import numpy as np

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# 데이터를 3차원으로 줄여서 예쁘게 시각화해주는 함수
# 고차원 데이터를 t-SNE로 3차원으로 줄여서 3D 산점도로 시각화하는 함수
"""
왜 쓰나?
수많은 차원의 복잡한 데이터를 사람이 이해하기 쉽게 보여주기 위해
군집/패턴/비슷한 것들끼리 모이는지 확인하기 위해
데이터 탐색(EDA)이나 논문 그림용으로도 자주 씀
"""
def tsne_plot(data):
    # t-SNE를 사용해 데이터를 3차원으로 축소
    tsne = TSNE(n_components=3, random_state=42, perplexity=data.shape[0] - 1)
    """
    perplexity = data.shape[0] - 1는 "데이터가 적으면 그냥 전부를 이웃으로 보고 펼치자"는 단순한 방법론
    권장하는 perplexity는 5 ~ 50
    """
    data_3d = tsne.fit_transform(data)

    # 그림과 3D 축 생성
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    # 각 포인트별 색상 할당 (인덱스 기반)
    num_points = len(data_3d)
    colors = plt.cm.tab20(np.linspace(0, 1, num_points))

    # 각 포인트를 고유 색상으로 산점도에 표시
    for idx, point in enumerate(data_3d):
        ax.scatter(point[0], point[1], point[2], label=str(idx), color=colors[idx])

    # 축 라벨과 제목 추가
    ax.set_xlabel('TSNE Component 1')
    ax.set_ylabel('TSNE Component 2')
    ax.set_zlabel('TSNE Component 3')
    plt.title('3D t-SNE Visualization')
    plt.legend(title='Input Order')
    plt.show()

# 파일 다운로드(1회용)
filename = 'companyPolicies.txt'
url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'

# Use wget to download the file
wget.download(url, out=filename)
print('file downloaded')
##

"""
회사 정책 텍스트 파일을 읽어서, 빈 줄을 제거하고 문단 단위 리스트로 나누는 함수
"""
def read_and_split_text(filename):
    # 파일을 UTF-8 인코딩으로 열어서 내용을 읽기
    with open(filename, 'r', encoding='utf-8') as file:
        text = file.read()

    # 읽은 텍스트를 줄바꿈(\n) 기준으로 나누어 문단 리스트 생성
    paragraphs = text.split('\n')

    # 빈 줄이나 공백만 있는 항목 제거하고 양쪽 공백 제거
    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]

    # 전처리된 문단 리스트 반환
    return paragraphs


# 텍스트 파일을 읽고 문단 단위로 분리
paragraphs = read_and_split_text('companyPolicies.txt')

# 처음 10개의 문단 출력
print(paragraphs[0:10])

# 받은거 뿌려보기
for i in range(4):
    print(f"sample: {i} paragraph: {paragraphs[i]} \n" )

# *****DPR 문맥 인코더용 토크나이저를 로드 (페이스북이 공개한 사전학습 모델)
context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')
print(context_tokenizer)

# 예시 쌍(질문, 답변) 형태의 텍스트
text = [("How are you?", "I am fine."), ("What's up?", "Not much.")]
print(text)

# *****위 텍스트를 토큰화 → 배치로 텐서 변환 (패딩/자르기 포함)
tokens_info = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
print(tokens_info)

# 토큰 ID를 실제 단어 토큰으로 변환해 보기
for s in tokens_info['input_ids']:
   print(context_tokenizer.convert_ids_to_tokens(s))

# *****DPR 문맥 인코더 모델 불러오기
context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')

# *****위에서 만든 문단을 뒤섞음
random.shuffle(paragraphs)

# *****상위 20개 샘플을 토큰화
tokens = context_tokenizer(paragraphs[:20], return_tensors='pt', padding=True, truncation=True, max_length=256)
print(tokens)

# *****DPR 컨텍스트 인코더를 통해 임베딩 추출
outputs = context_encoder(**tokens)
print(outputs.pooler_output)

# 3D t-SNE 시각화
tsne_plot(outputs.pooler_output.detach().numpy())

# 임의로 섞인 후 문단 샘플 16, 12 출력 (무작위 내용 확인)
print("sample 16:", paragraphs[16])
print("sample 12:", paragraphs[12])

# *****개별 문단 5개를 루프 돌며 인코딩
embeddings = []
for text in paragraphs[0:5]:
    # 한 문장씩 토큰화
    inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
    # 임베딩 생성
    outputs = context_encoder(**inputs)
    # 결과의 pooler_output을 저장
    embeddings.append(outputs.pooler_output)
    print("number of samples:")
    print(len(embeddings))
    print("samples shape:")
    print(outputs.pooler_output.shape)

# *****모든 개별 임베딩을 합쳐서 하나의 큰 배열로 → shape 확인
print(torch.cat(embeddings).detach().numpy().shape)

def encode_contexts(text_list):
    # 입력된 텍스트 리스트를 임베딩 벡터로 변환하는 함수
    embeddings = []
    for text in text_list:
        # 각 문장(문단)을 토큰화
        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)
        # DPR 컨텍스트 인코더로 임베딩 생성
        outputs = context_encoder(**inputs)
        # 풀러 출력(pooler_output)을 리스트에 추가
        embeddings.append(outputs.pooler_output)
    # 모든 벡터를 하나의 큰 배열로 이어붙여서 반환
    return torch.cat(embeddings).detach().numpy()

# 위 함수를 사용해서 paragraphs 리스트를 임베딩 벡터로 변환
context_embeddings = encode_contexts(paragraphs)

import faiss

# 임베딩 리스트를 단일 NumPy 배열로 변환
embedding_dim = 768  # 임베딩 차원 수 (모델 아키텍처에 맞춰야 함)
context_embeddings_np = np.array(context_embeddings).astype('float32')

# FAISS의 L2 거리 기반 평면 인덱스 생성
index = faiss.IndexFlatL2(embedding_dim)
# 문맥 임베딩들을 인덱스에 추가
index.add(context_embeddings_np)

# DPR 질문 인코더와 토크나이저 로드
question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')
question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')

# 예제 질문
question = 'Drug and Alcohol Policy'
# 질문을 토큰화
question_inputs = question_tokenizer(question, return_tensors='pt')
# 질문 인코더를 통해 임베딩 생성
question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()

# 생성된 질문 임베딩으로 FAISS 인덱스에서 유사한 문맥 벡터 검색 (상위 5개)
D, I = index.search(question_embedding, k=5)
print("D:", D)  # 거리(유사도) 정보
print("I:", I)  # 검색된 문맥의 인덱스

# 검색된 상위 5개의 문맥(문단)을 출력
print("Top 5 relevant contexts:")
for i, idx in enumerate(I[0]):
    # i+1번째 문단 내용과 거리(유사도) 값 출력
    print(f"{i+1}: {paragraphs[idx]}")
    print(f"distance {D[0][i]}\n")

def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5):
    """
    주어진 질문에 대해 가장 관련성이 높은 문맥들을 검색하는 함수.

    인자:
    - question: 질문 문장 (str)
    - question_tokenizer: 질문 인코더용 토크나이저
    - question_encoder: DPR 질문 인코더 모델
    - index: FAISS 인덱스
    - k: 상위 몇 개의 문맥을 반환할지

    반환값:
    - tuple: (거리 행렬, 인덱스 행렬)
    """

    # 질문을 토큰화
    question_inputs = question_tokenizer(question, return_tensors='pt')

    # 질문을 인코더를 통해 임베딩 벡터로 변환
    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()

    # FAISS 인덱스에서 질문 임베딩과 가장 유사한 k개의 문맥 검색
    D, I = index.search(question_embedding, k)

    return D, I

# GPT2 토크나이저와 모델 불러오기 (Hugging Face에서 제공하는 공개 커뮤니티 버전)
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

# 생성 시 패딩 토큰 ID 설정 (에러 방지용)
model.generation_config.pad_token_id = tokenizer.pad_token_id

# 단일 문장 예제 (요약/생성 테스트용)
contexts = "What is a large language model?"

# GPT2 토크나이저로 입력 문장 토큰화 (패딩/자르기 포함)
inputs = tokenizer(contexts, return_tensors='pt', max_length=1024, truncation=True)
print(inputs)  # 토큰화된 입력 텐서 출력

# GPT2 모델을 사용해 텍스트 생성 (빔서치 사용, 최대 길이 50)
summary_ids = model.generate(
    inputs['input_ids'],
    max_length=50,
    num_beams=4,
    early_stopping=True,
    pad_token_id=tokenizer.eos_token_id
)
print(summary_ids)  # 생성된 토큰 ID 시퀀스 출력

# 생성된 토큰 시퀀스를 사람이 읽을 수 있는 문자열로 디코딩
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(summary)  # 최종 생성된 텍스트 출력


def generate_answer_without_context(question):
    """
    추가 문맥(컨텍스트) 없이 질문 문장만으로 GPT2 모델이 직접 답변 생성.

    인자:
    - question: str, 질문 문장

    반환:
    - answer: str, 생성된 답변 텍스트
    """
    # 입력 질문을 토큰화
    inputs = tokenizer(question, return_tensors='pt', max_length=1024, truncation=True)

    # 질문만을 기반으로 GPT2가 직접 생성
    summary_ids = model.generate(
        inputs['input_ids'],
        max_length=150,  # 생성될 최대 토큰 수
        min_length=40,  # 생성될 최소 토큰 수
        length_penalty=2.0,  # 생성 길이에 대한 페널티 (더 긴 텍스트 유도)
        num_beams=4,  # 빔 서치 사용 (더 높은 품질)
        early_stopping=True,  # 빔 서치 조기 종료 허용
        pad_token_id=tokenizer.eos_token_id  # 패딩 토큰 설정
    )

    # 토큰 ID 시퀀스를 사람이 읽을 수 있는 문자열로 변환
    answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return answer

# 사용 예제
question = "what is mobile policy?"
answer = generate_answer_without_context(question)

print("Answer:", answer)  # 생성된 답변 출력

def generate_answer(question, contexts):
    # 검색된 컨텍스트들을 하나의 큰 문자열로 이어서 GPT2 입력으로 만들기
    input_text = question + ' ' + ' '.join(contexts)

    # 입력 텍스트를 토큰화 (패딩/자르기 포함)
    inputs = tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)

    # GPT2를 사용해 답변 생성
    summary_ids = model.generate(
        inputs['input_ids'],
        max_new_tokens=50,  # 새로 생성할 최대 토큰 수
        min_length=40,  # 생성될 최소 길이
        length_penalty=2.0,  # 더 긴 답변을 선호하도록 페널티 조정
        num_beams=4,  # 빔 서치 사용해 더 좋은 결과 탐색
        early_stopping=True,  # 조건 만족하면 빔 서치 조기 종료
        pad_token_id=tokenizer.eos_token_id  # 패딩 토큰 설정
    )

    # 생성된 토큰 시퀀스를 사람이 읽을 수 있는 텍스트로 변환
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# 예제 질문
question = "what is mobile policy?"

# DPR 검색 함수로 가장 관련 있는 문단 상위 5개 찾기
_, I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)

print(f"paragraphs indexs {I}")  # 검색된 문단들의 인덱스 출력

# 검색 결과 인덱스를 사용해 실제 문단 텍스트 가져오기
top_contexts = [paragraphs[idx] for idx in I[0]]
print(f"top_contexts {top_contexts}")  # 선택된 문단 내용 출력

# 질문과 상위 컨텍스트들을 GPT2에 넣어 최종 답변 생성
answer = generate_answer(question, top_contexts)
print("Generated Answer:", answer)  # 생성된 답변 출력